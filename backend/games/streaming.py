"""
Streaming module for video feeds with MediaPipe/DeepFace processing.
CLEAN VIDEO - No text overlays on video. Status sent separately.
"""
import cv2
import mediapipe as mp
import math
import time
import numpy as np
from collections import deque, Counter
import threading
import json
try:
    from .utils import OneEuroFilter
except ImportError:
    try:
        from games.utils import OneEuroFilter
    except ImportError:
        from utils import OneEuroFilter

# Global state for each stream type
stream_states = {
    "gesture": {"status": "waiting", "gesture": "None", "message": "Show your hand"},
    "pose": {"status": "waiting", "pose": "None", "message": "Step into frame"},
    "emotion": {"status": "waiting", "emotion": "neutral", "message": "Analyzing..."}
}
state_lock = threading.Lock()

# ============== STREAMING CONFIG ==============
# Uses MediaPipe (Solutions) for all tracking
# - Hands: GestureStream
# - Pose: PoseStream (Holistic)
# - Emotion: ExpressionLab (FaceMesh)


# ============== HAND GESTURE STREAM (CLEAN) ==============
class HandGestureStream:
    def __init__(self):
        try:
            self.mp_hands = mp.solutions.hands
            self.mp_draw = mp.solutions.drawing_utils
            self.mp_styles = mp.solutions.drawing_styles
        except AttributeError:
            import mediapipe.python.solutions as solutions
            mp.solutions = solutions
            self.mp_hands = mp.solutions.hands
            self.mp_draw = mp.solutions.drawing_utils
            self.mp_styles = mp.styles

        self.hands = self.mp_hands.Hands(
            max_num_hands=2, # Support both hands
            min_detection_confidence=0.7, 
            min_tracking_confidence=0.6,
            model_complexity=1
        )
        # Tracking for Swipe detection per hand
        self.histories = {} # Dict: hand_label -> deque
        self.filters_x = {} # Dict: hand_label -> OneEuroFilter
        self.filters_y = {} # Dict: hand_label -> OneEuroFilter
        
        self.last_swipe_time = 0
        self.swipe_cooldown = 1.0 # 1 second cooldown across all hands

    def process_frame(self, frame):
        h, w, c = frame.shape
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(img_rgb)
        
        gesture = "None"
        message = "Show your hand to the camera"
        
        current_time = time.time()
        detected_gestures = []

        if results.multi_hand_landmarks and results.multi_handedness:
            for idx, hand_lms in enumerate(results.multi_hand_landmarks):
                # Identify hand (Left/Right)
                hand_label = results.multi_handedness[idx].classification[0].label # 'Left' or 'Right'
                
                # Draw hand landmarks
                self.mp_draw.draw_landmarks(
                    frame, 
                    hand_lms, 
                    self.mp_hands.HAND_CONNECTIONS,
                    self.mp_styles.get_default_hand_landmarks_style(),
                    self.mp_styles.get_default_hand_connections_style()
                )
                
                # Finger tracking
                raw_idx_x = int(hand_lms.landmark[8].x * w)
                raw_idx_y = int(hand_lms.landmark[8].y * h)
                
                # Initialize filters/history for this hand if not exists
                if hand_label not in self.filters_x:
                    self.filters_x[hand_label] = OneEuroFilter(current_time, raw_idx_x, min_cutoff=1.0, beta=0.01)
                    self.filters_y[hand_label] = OneEuroFilter(current_time, raw_idx_y, min_cutoff=1.0, beta=0.01)
                    self.histories[hand_label] = deque(maxlen=8) # Slightly shorter history for snappier response

                # Apply smoothing filters
                idx_x = int(self.filters_x[hand_label].filter(current_time, raw_idx_x))
                idx_y = int(self.filters_y[hand_label].filter(current_time, raw_idx_y))

                thumb_x = int(hand_lms.landmark[4].x * w)
                thumb_y = int(hand_lms.landmark[4].y * h)
                dist = math.hypot(idx_x - thumb_x, idx_y - thumb_y)
                
                # Update history for this hand
                self.histories[hand_label].append(idx_x)
                
                # Swipe Logic for this hand
                history = self.histories[hand_label]
                if len(history) == history.maxlen and (current_time - self.last_swipe_time > self.swipe_cooldown):
                    diff = history[-1] - history[0]
                    
                    # Directional Consistency
                    is_monotonic = True
                    noise_margin = w * 0.03
                    for i in range(1, len(history)):
                        if diff > 0 and history[i] < history[i-1] - noise_margin:
                            is_monotonic = False
                            break
                        if diff < 0 and history[i] > history[i-1] + noise_margin:
                            is_monotonic = False
                            break
                    
                    # Deliberate movement threshold
                    threshold = w * 0.25 # Lower threshold (was 0.35) for better sensitivity
                    
                    if abs(diff) > threshold and is_monotonic:
                        # Toward User's Left (Screen Right) = diff > 0 = Next Slide
                        # Toward User's Right (Screen Left) = diff < 0 = Prev Slide
                        hand_gesture = "Swipe_Left" if diff > 0 else "Swipe_Right"
                        detected_gestures.append((hand_gesture, abs(diff)))
                        print(f"âœ… {hand_label} SLAP: diff={diff}, gesture={hand_gesture}")

                # Draw feedback (Pinch Click)
                if dist < 40:
                    cv2.circle(frame, (idx_x, idx_y), 20, (0, 255, 0), cv2.FILLED)
                    if gesture == "None": gesture = "Pinch Click"
                else:
                    color = (255, 0, 255) if hand_label == "Right" else (0, 255, 255)
                    cv2.circle(frame, (idx_x, idx_y), 15, color, cv2.FILLED)

        # Process detected gestures
        if detected_gestures:
            # Sort by movement intensity and take the strongest
            detected_gestures.sort(key=lambda x: x[1], reverse=True)
            gesture = detected_gestures[0][0]
            
            if gesture == "Swipe_Left":
                message = "âž¡ï¸ Next Slide"
            elif gesture == "Swipe_Right":
                message = "â¬…ï¸ Prev Slide"
            
            self.last_swipe_time = current_time
            # Clear all histories to prevent double-fire
            for h_label in self.histories:
                self.histories[h_label].clear()

        # Update global state
        with state_lock:
            stream_states["gesture"] = {
                "status": "active" if gesture != "None" else "waiting",
                "gesture": gesture,
                "message": message
            }
        
        return frame        # Persistent history through brief occlusion
        
        # Update global state
        with state_lock:
            stream_states["gesture"] = {
                "status": "active" if gesture != "None" else "waiting",
                "gesture": gesture,
                "message": message
            }
        
        return frame


# ============== POSE STREAM (HOLISTIC - BODY + HANDS) ==============
class PoseStream:
    def __init__(self):
        try:
            self.mp_holistic = mp.solutions.holistic
            self.mp_draw = mp.solutions.drawing_utils
            self.mp_styles = mp.solutions.drawing_styles
        except AttributeError:
            import mediapipe.python.solutions as solutions
            mp.solutions = solutions
            self.mp_holistic = mp.solutions.holistic
            self.mp_draw = mp.solutions.drawing_utils
            self.mp_styles = mp.solutions.drawing_styles

        # Use Holistic model for Body + Hands + Face
        self.holistic = self.mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
            model_complexity=1,
            smooth_landmarks=True
        )
        
        # Custom drawing specs for high visibility
        self.pose_landmark_spec = self.mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)
        self.pose_connection_spec = self.mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)
        self.hand_landmark_spec = self.mp_draw.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=2)
        self.hand_connection_spec = self.mp_draw.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=2)

    def process_frame(self, frame):
        h, w, c = frame.shape
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.holistic.process(img_rgb)
        
        pose_status = "None"
        message = "Step back so I can see you"
        
        if results.pose_landmarks:
            # Draw Pose (Body)
            self.mp_draw.draw_landmarks(
                frame,
                results.pose_landmarks,
                self.mp_holistic.POSE_CONNECTIONS,
                landmark_drawing_spec=self.pose_landmark_spec,
                connection_drawing_spec=self.pose_connection_spec
            )
            
            # Draw Left Hand
            self.mp_draw.draw_landmarks(
                frame,
                results.left_hand_landmarks,
                self.mp_holistic.HAND_CONNECTIONS,
                landmark_drawing_spec=self.hand_landmark_spec,
                connection_drawing_spec=self.hand_connection_spec
            )

            # Draw Right Hand
            self.mp_draw.draw_landmarks(
                frame,
                results.right_hand_landmarks,
                self.mp_holistic.HAND_CONNECTIONS,
                landmark_drawing_spec=self.hand_landmark_spec,
                connection_drawing_spec=self.hand_connection_spec
            )

            # Draw Face Mesh (Lightweight)
            self.mp_draw.draw_landmarks(
                frame,
                results.face_landmarks,
                self.mp_holistic.FACEMESH_TESSELATION,
                landmark_drawing_spec=None,
                connection_drawing_spec=self.mp_styles.get_default_face_mesh_tesselation_style()
            )
            
            # Logic for pose detection (same as before)
            landmarks = results.pose_landmarks.landmark
            nose = landmarks[self.mp_holistic.PoseLandmark.NOSE]
            left_wrist = landmarks[self.mp_holistic.PoseLandmark.LEFT_WRIST]
            right_wrist = landmarks[self.mp_holistic.PoseLandmark.RIGHT_WRIST]
            left_shoulder = landmarks[self.mp_holistic.PoseLandmark.LEFT_SHOULDER]
            right_shoulder = landmarks[self.mp_holistic.PoseLandmark.RIGHT_SHOULDER]
            
            if left_wrist.y < nose.y and right_wrist.y < nose.y:
                pose_status = "Hands Up"
                message = "ðŸ™Œ Both hands raised!"
            elif (abs(left_wrist.y - left_shoulder.y) < 0.15 and 
                  abs(right_wrist.y - right_shoulder.y) < 0.15):
                pose_status = "T-Pose"
                message = "âœˆï¸ T-Pose detected!"
            elif left_wrist.y < left_shoulder.y and right_wrist.y > right_shoulder.y:
                pose_status = "Left Raised"
                message = "ðŸ‘ˆ Left arm raised"
            elif right_wrist.y < right_shoulder.y and left_wrist.y > left_shoulder.y:
                pose_status = "Right Raised"
                message = "ðŸ‘‰ Right arm raised"
            else:
                pose_status = "Standing"
                message = "ðŸ§ Standing position"
        
        # Update global state
        with state_lock:
            stream_states["pose"] = {
                "status": "active" if pose_status != "None" else "waiting",
                "pose": pose_status,
                "message": message
            }
        
        return frame


# ============== EXPRESSION LAB (Advanced MediaPipe Logic) ==============
class EmotionStream:
    def __init__(self):
        try:
            # Import MediaPipe solutions
            self.mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                max_num_faces=1,
                refine_landmarks=True, # Critical for accurate eyes/lips
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
        except Exception as e:
            print(f"MediaPipe FaceMesh init failed: {e}")
            self.face_mesh = None

        self.emotion = "neutral"
        # Scores initialized to 0
        self.scores = {
            "happy": 0.0,
            "sad": 0.0,
            "surprise": 0.0,
            "angry": 0.0,
            "fear": 0.0,
            "natural": 0.0
        }
        # Smoothing factor (0.0 - 1.0). Lower = smoother but slower.
        self.alpha = 0.2

    def get_pt(self, landmarks, idx):
        # Return 3D point (x, y, z)
        return np.array([landmarks[idx].x, landmarks[idx].y, landmarks[idx].z])

    def dist(self, p1, p2):
        return np.linalg.norm(p1 - p2)

    def process_frame(self, frame):
        if not self.face_mesh:
            return frame

        h, w, c = frame.shape
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(img_rgb)
        
        current_scores = {k: 0.0 for k in self.scores}
        
        if results.multi_face_landmarks:
            lm = results.multi_face_landmarks[0].landmark
            
            # Helper to get numpy point
            p = lambda i: np.array([lm[i].x, lm[i].y, lm[i].z]) # 3D
            # Distance helper (3D)
            d = lambda i, j: np.linalg.norm(p(i) - p(j))

            # --- DIMENSIONS (Normalization) ---
            face_width = d(454, 234) # Ear to ear
            face_height = d(10, 152) # Top to chin
            
            # Normalize all ratios by face_width to be scale-invariant
            norm = face_width
            
            # --- FEATURE 1: MOUTH ---
            mouth_w = d(61, 291)
            mouth_h = d(13, 14) 
            mouth_open_ratio = mouth_h / mouth_w if mouth_w > 0 else 0
            
            # Smile Curve (2D projection is safer for up/down curve, but we check Width)
            # Use raw Y for curve because Z affects depth, but Y is "up/down" on screen
            corners_y = (lm[61].y + lm[291].y) / 2.0
            center_y = (lm[0].y + lm[17].y) / 2.0
            smile_curve = (center_y - corners_y) * 100 
            
            # Smile Width Ratio (Mouth width / Face width)
            mouth_width_ratio = mouth_w / norm

            # --- FEATURE 2: BROWS ---
            # Brow Height (avg distance from eye top to brow)
            brow_L_h = d(159, 66)
            brow_R_h = d(386, 296)
            avg_brow_h = (brow_L_h + brow_R_h) / 2.0
            brow_lift_ratio = avg_brow_h / norm
            
            # Brow Furrow (Distance between inner brows)
            brow_inner_dist = d(66, 296)
            brow_furrow_ratio = brow_inner_dist / norm

            # --- FEATURE 3: EYES ---
            eye_L_h = d(159, 145)
            eye_R_h = d(386, 374)
            avg_eye_h = (eye_L_h + eye_R_h) / 2.0
            eye_open_ratio = avg_eye_h / norm

            # --- LOGIC & SCORING ---
            
            # HAPPY
            # Needs UP curve AND WIDE mouth (prevents head tilt false positives)
            # Natural mouth width ratio is usually ~0.35. Smile is > 0.40
            happy_score = 0
            if smile_curve > 0.5:
                # Only trust curve if mouth is actually wide
                if mouth_width_ratio > 0.42: 
                    happy_score = smile_curve * 50
                elif mouth_width_ratio > 0.38:
                    happy_score = smile_curve * 30
            current_scores["happy"] = min(100, max(0, happy_score)) 
            
            # ANGRY
            # Brows LOW (small lift) OR Brows FURROWED (small inner dist)
            angry_score = 0
            # Brow Furrow is very strong indicator for anger
            if brow_furrow_ratio < 0.16: # Brows touching
                angry_score += 60
            elif brow_furrow_ratio < 0.18:
                angry_score += 30
                
            # Squinting eyes + low brows
            if eye_open_ratio < 0.04 and brow_lift_ratio < 0.15:
                angry_score += 40
            
            current_scores["angry"] = min(100, angry_score)

            # SURPRISE
            # High brows + Open mouth + Open eyes
            surprise_score = 0
            if mouth_open_ratio > 0.3 and eye_open_ratio > 0.06:
                surprise_score += 50
            if brow_lift_ratio > 0.25: # High brows
                surprise_score += 40
            current_scores["surprise"] = min(100, surprise_score)

            # SAD
            # Down curve + Brows strictly NOT furrowed (to distinguish from angry)
            sad_score = 0
            if smile_curve < -1.0 and brow_furrow_ratio > 0.20:
                sad_score = abs(smile_curve) * 40
            current_scores["sad"] = min(100, sad_score)
            
            # FEAR
            # Often confused with surprise: Wide eyes + Open mouth + Brows specific
            # Fear has brows raised but flatter? Hard to distinguish simply.
            fear_score = 0
            if eye_open_ratio > 0.08 and mouth_open_ratio > 0.2:
                 # Slightly different tuning than surprise
                 if brow_lift_ratio < 0.25: # Brows NOT super high
                     fear_score = 40
            current_scores["fear"] = fear_score

            # NATURAL
            # High score if everything else is low
            max_other = max(current_scores.values())
            # Boost natural if neutral face features are met
            if mouth_width_ratio < 0.40 and abs(smile_curve) < 1.0 and eye_open_ratio > 0.04:
                current_scores["natural"] = 100 - (max_other * 0.8)
            else:
                current_scores["natural"] = max(0, 100 - max_other)

            # --- TEMPORAL SMOOTHING ---
            for k in self.scores:
                # EMA
                self.scores[k] = (self.alpha * current_scores.get(k, 0)) + ((1 - self.alpha) * self.scores[k])

            # Find dominant
            self.emotion = max(self.scores, key=self.scores.get)
            
        else:
            # Decay all scores if face lost
            for k in self.scores:
                self.scores[k] *= 0.9
            if max(self.scores.values()) < 10:
                self.emotion = "neutral"

        # Mapping to friendly UI messages
        emotion_messages = {
            "happy": "ðŸ˜Š You look radiant!",
            "sad": "ðŸ˜¢ It's okay to be sad.",
            "surprise": "ðŸ˜² Did something shock you?",
            "angry": "ðŸ˜  Easy there, tiger.",
            "fear": "ðŸ˜¨ You're safe here.",
            "natural": "âœ¨ Natural & Relaxed",
            "neutral": "âœ¨ Natural & Relaxed" 
        }
        
        message = emotion_messages.get(self.emotion, "Analyzing...")
        
        # Prepare scores for UI (convert to int)
        ui_scores = {k: float(v) for k, v in self.scores.items() if v > 1.0}

        # Update global state
        with state_lock:
            stream_states["emotion"] = {
                "status": "active",
                "emotion": self.emotion,
                "scores": ui_scores,
                "message": message
            }
        
        return frame

    def _analysis_complete(self, future):
        # Deprecated in this version
        pass


# ============== STREAM GENERATOR (MAXIMUM QUALITY) ==============
def generate_stream(stream_type: str):
    """
    Generator function that yields MJPEG frames.
    MAXIMUM QUALITY - Uses native camera resolution and MJPEG codec.
    """
    cap = cv2.VideoCapture(0)
    
    # CRITICAL: Set MJPEG codec FIRST before resolution (best practice)
    # This enables hardware-accelerated MJPEG which gives better quality and FPS
    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))
    
    # Request highest possible resolution (camera will use max it supports)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    # Disable auto-focus hunting if supported (reduces blur)
    cap.set(cv2.CAP_PROP_AUTOFOCUS, 1)
    
    # Get actual resolution (camera may not support requested)
    actual_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    print(f"Camera streaming at {actual_w}x{actual_h}")
    
    # Initialize the appropriate processor
    if stream_type == "gesture":
        processor = HandGestureStream()
    elif stream_type == "pose":
        processor = PoseStream()
    elif stream_type == "emotion":
        processor = EmotionStream()
    else:
        processor = None
    
    try:
        while True:
            success, frame = cap.read()
            if not success:
                break
            
            frame = cv2.flip(frame, 1)  # Mirror
            
            if processor:
                frame = processor.process_frame(frame)
            
            # MAXIMUM QUALITY JPEG (100% = no compression)
            ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 100])
            if not ret:
                continue
            
            yield (
                b'--frame\r\n'
                b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n'
            )
    finally:
        cap.release()


def get_stream_status(stream_type: str) -> dict:
    """Get the current status for a stream type."""
    with state_lock:
        return stream_states.get(stream_type, {"status": "unknown"})
